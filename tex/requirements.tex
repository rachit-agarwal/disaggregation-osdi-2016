\section{Network Requirements}
\label{sec:requirements}
We start by evaluating network latency and bandwidth requirements for disaggregation.
We describe our evaluation methodology (\S \ref{ssec:rmethod}), present our results (\S \ref{ssec:rr}) and then discuss their implications (\S \ref{ssec:rtt}). 

\subsection{Methodology}
\label{ssec:rmethod}
In DDC, traffic between resources that was contained within a server is now carried on the ``external'' network. As with other types of interconnects, the key requirement will be low latency and high throughput to enable this disaggregation. We review the forms of communication between resources within a server in Table~\ref{tab:tech} to examine the feasibility of such a network. As mentioned in \S\ref{sec:summary}, CPU-to-CPU cache coherence traffic does not cross the external network.
For I/O traffic to storage devices, the current latency and bandwidth requirements are such that we can expect to consolidate them into the network fabric with low performance impact. 
Thus, the dominant impact to application performance will come from CPU-memory disaggregation; hence, we focus on evaluating the network bandwidth and latency required to support remote memory. 

As mentioned earlier, we assume that remote memory is managed at the page granularity, in conjunction with virtual memory page replacement algorithms implemented by the hypervisor or operating system.
For each paging operation there are two main sources 
of performance penalty: i) the software overhead for trap and page eviction and ii) the time to transfer pages over the network. 
Given our focus on network requirements, we only consider the latter in this paper (modulo a brief discussion on current 
software overheads later in this section).

\paragraphb{Applications and Testbed}
% We conduct a series of experiments to examine how network latency and bandwidth affect application performance with remote memory accesses (which we emulate as described below). 
We use workloads from diverse applications running on real-world and benchmark datasets, as shown in Table~\ref{tab:workloads}. We take these applications as is, rather than seek to optimize them for \dis; this represents a worst-case for the network, as \dis-driven application modifications could lessen the network requirements. %These workloads constitute a wide variety of application domains including off-disk batch processing (Hadoop), in-memory batch processing (Spark), graph processing (GraphLab) and point queries (memcached).
Each application operates on $\sim125$GB of data equally distributed across an Amazon EC2 cluster comprising of $5$ m3.2xlarge servers. Each of these servers has $8$ vCPUs, $30$GB main memory, $2 \times 80$GB SSD drives and a $1$Gbps access link bandwidth.
We enabled EC2's Virtual Private Network (VPC~\cite{vpc}) capability in our cluster to ensure no interference with other Amazon EC2 instances. 
 
We verified that m3.2xlarge instances' $1$Gbps access links were not a bottleneck to our experiment in two ways. 
First, we considered CPU utilization. In all cases where the network approached full utilization, CPU was highly utilized, indicating that the CPU was not blocked on network calls. 
Next, we ran our testbed on c3.4xlarge instances with $2$Gbps access links (increased network bandwidth with roughly the same CPU). We verified that even with more bandwidth, all applications for which link utilization was high maintained high CPU utilization. 

%
\begin{figure*}

  \centering
  \subfigure{
    \includegraphics[width=\textwidth]{img/vary_latency_bw.eps}  
  }

  \subfigure{
    \includegraphics[width=\textwidth]{img/vary_latency_bw_high.eps}
  }
  \caption{\small{Comparison of application-level performance in disaggregated datacenters with respect to existing server-centric architectures for different latency/bandwidth configurations and $25\%$ local memory on CPU blades --- Dolphins (top) and Sharks (bottom). To maintain application-level performance within reasonable performance bounds ($\sim 5\%$ on an average), Dolphins require $5\mu$s end-to-end latency and $40$Gbps bandwidth, and Sharks require  $3\mu$s end-to-end latency and $40-100$Gbps bandwidth. See \S\ref{ssec:rr} for detailed discussion.}}
  \label{fig:latb}
\end{figure*}
%
\paragraphb{Emulating remote memory} 
When running the above workloads, we emulate remote memory accesses by implementing a special swap device that is backed by physical memory rather than a disk. This effectively partitions main memory into a ``local'' and ``remote'' portion 
with existing page replacement algorithms controlling when and how pages are transferred between the two; we tune the amount of ``local'' memory by configuring the size of the swap device.
We intercept all page faults and inject artificial delays to emulate network round-trip latency and bandwidth for each paging operation. 

We measure relative application-level performance on the basis of job completion time as compared to the zero-delay case. Thus, our results do not account for the delay introduced by  software overheads for page operations 
and should be interpreted as relative performance degradation over different network configurations. %, not the absolute performance of disaggregation.
Note too that the delay we inject is purely an artificial parameter and hence does not (for example) realistically model queueing delays that may result from network congestion caused by the extra traffic due to disaggregation; we consider network-wide traffic and effects such as congestion in \S\ref{sec:existing}.
To compensate the performance noise on EC2, we run each experiment 10 times and take the median result.



\subsection{Results}
\label{ssec:rr}
We start by evaluating application performance in a disaggregated vs. a server-centric architecture. 
Figure~\ref{fig:latb} plots the performance degradation for each application under different assumptions about the latency and bandwidth to remote memory. In these experiments, we set the local memory in the disaggregated scenario to be  $25\%$ of that in the server-centric case (we will examine our choice of 25\% shortly). 
%Intuitively, we would expect these applications to observe significant performance degradation on disaggregated hardware due to CPU-memory disaggregation.
%Figure~\ref{fig:latb} shows that this is not necessarily the case, if the network fabric can support $5\mu$s end-to-end latency and $40$Gbps access link bandwidth. 

%the performance degradation that results with nine different latency/bandwidth combinations, given 25\% of memory capacity as local.
%\footnote{The small discrepancies between Figures \ref{fig:impb} and \ref{fig:latb} are due to the small number of runs used in Figure \ref{fig:impb}.} 
% of the measured memory footprint for each workload. 
%We now evaluate the application-level performance for various \dis configurations in terms of end-to-end network latency and access link bandwidth at the end hosts (see Figure~\ref{fig:latb}). 

From Figure~\ref{fig:latb}, we see that our applications can be broadly divided into two categories based on the network latency and bandwidth needed to achieve a low performance penalty.
For example, for the applications in Fig.~\ref{fig:latb} (top) --- Hadoop Wordcount, Hadoop Sort, Graphlab and Memcached --- a network with an end-to-end latency of $5\mu$s and bandwidth of $40$Gbps is sufficient to maintain an average performance penalty under 5\%. 
In contrast, the applications in in Fig.~\ref{fig:latb} (bottom) --- Spark Wordcount, Spark Sort, Timely and SparkSQL BDB --- require network latencies of $3\mu$s and $40-100$Gbps bandwidth to maintain an average performance penalty under 8\%. We term the former applications {\em dolphins} and the latter {\em sharks} reflecting their more relaxed vs. demanding natures and examine the feasibility of meeting their respective requirements in \S\ref{ssec:rtt}.


%The dolphin applications are the ones that observe less than $5\%$ performance degradation (within the application performance variance) with a network of $5\mu$s end-to-end latency and $40$Gbps access link bandwidth. 
%Intuitively, the dolphin applications are the ones that are either not optimized to use available memory aggressively (Hadoop wordcount, Hadoop terasort) or are bottlenecked by network I/O (GraphLab, Memcached). Thus, CPU-memory disaggregation does not lead to significant performance degradation.

%Shark applications, on the other hand, exploit the available memory more aggressively and have stringent latency requirements to be able to maintain the performance similar to existing server-centric architectures. 
%Specifically, the shark applications require $3\mu$s end-to-end latency and $40$Gbps access link bandwidth to achieve negligible performance degradation.

%We examine the discuss the implications (and feasibility) of the end-to-end latency and bandwidth requirements of the dolphin and shark applications in \S\ref{ssec:rtt}. Prior to that, we discuss how various parameters may impact our conclusions.

% We make three observations. First, given a network with $5-10\mu$s end-to-end latency and $40$Gbps access link bandwidth, the application-level performance in \dis is within $5\%$ of that in current server-centric datacenters. 
% Second, as shown in Figure \ref{fig:impl}, fixing bandwidth at 40Gbps, the application performance is sensitive to the increase of end-to-end latency.
% Third, increasing the bandwidth to $100$Gbps, while ensuring $5\mu$s end-to-end network latency further reduces any performance degradation to within $2$--$3\%$ (Figure \ref{fig:impbw}).
% Thus, we conclude that \emph{\dis should aim for an end-to-end network latency and bandwidth of $5-10\mu$s and $40$Gbps respectively.}

%\paragraphb{Impact of local memory size}

% of the available main memory{\footnote{The $100\%$ local CPU cache size corresponds to no CPU-memory disaggregation, as in server-centric architectures.} \rc{Shall we say that we use this as our baseline, rather than application-level performance without SIT, to incorporate SIT overhead. (a+x)/(b+x) problem.}}}. 
%  shows that the application-level performance in \dis is within reasonable bounds (roughly $5$\%) of that in a server-centric architecture as long as the local memory cache is no smaller than $25\%$ of main memory. For larger local cache sizes, performance only improves slowly and for any smaller cache size, the performance degrades significantly. 
% We conclude that \emph{a local memory cache that is sized at $25\%$ of current main memory is sufficient} to maintain reasonable application-level performance in \dis. In what follows, unless stated otherwise, we set the local memory size to this value.

\paragraphb{Sensitivity analysis}
Next, we evaluate the sensitivity of application performance to network bandwidth and latency. Fig.~\ref{fig:impbw} plots the performance degradation under increasing network bandwidth assuming a fixed network latency of $5\mu$s while Fig.~\ref{fig:impl} plots degradation under increasing latency for a fixed bandwidth of $40$Gbps; in both cases, local memory is set at $25\%$ as before.
%nd end-to-end latency to be  and varying the bandwidth from $10$Gbps to $100$Gbps (Figure~\ref{fig:impbw}). 

We see that beyond $40$Gbps, increasing network bandwidth offers little improvement in application-level performance. 
\cut{ 
% move to later. 
This result is in sharp contrast with existing disaggregated hardware proposals that implicitly assume the necessity of high bandwidth links using non-commodity network components (\eg, Silicon photonics~\cite{firebox, hptm, rsa}, or PCIe links~\cite{huawei}); our evaluation suggests that, for existing applications and workloads, commodity network components with $40-100$Gbps bandwidth links may be sufficient. 
}
In contrast, performance --- particularly for shark applications --- is very sensitive to network latency; very low latencies ($3-5\mu$s) are needed to avoid non-trivial performance degradation.

%
\begin{figure}
  \centering
    \includegraphics[width = \columnwidth]{img/fix_latency_vary_bw.eps} 
  \caption{\small{Impact of network bandwidth on the results of Figure~\ref{fig:latb} for end-to-end latency fixed to $5\mu$s and local memory fixed to $25\%$. Beyond $40$Gbps, the network bandwidth has little or no impact on relative (disaggregated versus server-centric) application-level performance.}}
  \label{fig:impbw}
\end{figure}
%
%
\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{img/fix_bw_vary_latency.eps} 
  \caption{\small{Impact of network latency on the results of Figure~\ref{fig:latb} for bandwidth fixed to $40$Gbps and local memory fixed to $25\%$. Sharks are extremely sensitive to network latencies, while dolphins remains largely insensitive.}}
  \label{fig:impl}
\end{figure}
%
Finally, we measure how the amount of local memory impacts application performance.
%The result of Figure~\ref{fig:latb} assumed that each CPU blade retains $25\%$ of memory capacity as local cache. We now evaluate how amount of local cache impacts the application-level performance (and hence, our conclusions) above. 
Figure~\ref{fig:impb} plots the performance degradation that results as we vary the 
fraction of local memory from $100\%$ (which corresponds to no CPU-memory disaggregation) down to $10\%$, assuming a fixed network latency and bandwidth of $5\mu$s and $40$Gbps respectively; note that the $25\%$ values in Figure~\ref{fig:impb} correspond to $5\mu$s, $40$Gbps results in Figure~\ref{fig:latb}. 
As expected, we see that shark applications are more sensitive to the amount of local memory than dolphins; e.g., increasing the amount of local memory from $20\%$ to $30\%$ roughly halves the performance degradation in sharks from approximately $15\%$ to $7\%$.
%even for $5\mu$s end-to-end latency. 
%In contrast, dolphin applications are far less sensitive to the amount of local memory. 
In all cases, increasing the amount of local memory beyond $40\%$ has little to no impact on performance degradation.

\paragraphb{Summary of results} 
In summary, supporting memory disaggregation while maintaining application-level performance within reasonable bounds imposes certain requirements on the network in terms of the end-to-end latency and bandwidth it must provide. Moreover, these requirements are closely related to the amount of local memory available to CPU blades. Table~\ref{tab:rmem} summarizes these requirements for the applications we studied. We specifically investigate a few combinations of network latency, bandwidth, and the amount of local memory needed to maintain a performance degradation under 5\%. 
We highlight these design points because they represent what we consider to be sweet spots in achievable targets both for the amount of local memory and for network requirements, as we discuss next.



%Next, we examine the feasibility of meeting the network requirements.

\cut{ 
to avoid  (has a more significant impact of application-level performance (Figure~\ref{fig:impl}). In particular, while shark application performance is extremely sensitive to remote memory access latency, the performance for dolphin applications can also be impacted significantly due to high latency. We use this observation below to discuss the implications and feasibility of networks for disaggregated datacenters.

\paragraphb{Impact of CPU blade density}
We start our evaluation by studying the impact of local memory on application-level performance. \rc{I am thinking about this; not sure how larger number of cores would impact the number of memory accesses!}

\paragraphb{Impact of data sizes}
We start our evaluation by studying the impact of local memory on application-level performance. \rc{I am thinking about a concise way to discuss this issue --- for Hadoop, Spark it may be easy (the number of tasks/waves increase proportionally, things should not change); for memcached and SparkSQL, unclear}

Overall, we conclude that \emph{it is possible to achieve resource disaggregation with little or no impact of application-level performance if the underlying network fabric can provide an end-to-end latency of $5\mu$s (for dolphin applications) and of $3\mu$s (for shark applications) along with a $40$Gbps bandwidth links}. 
} 

%
\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{img/vary_remote_mem.eps} 
  \caption{\small{Impact of ``local memory'' on the results of Figure~\ref{fig:latb} for end-to-end latency fixed to $5\mu$s and network bandwidth fixed to $40$Gbps. Local memory has significant impact on the performance of sharks and noticeable impact on dolphins (at least until $40\%$). Beyond $35\%$ local memory, both sharks and dolphins achieve performance within reasonable bounds. This could be reduced to $30\%$ for lower latency and/or higher bandwidth configurations (see Table~\ref{tab:rmem}).}}
  \label{fig:impb}
\end{figure}
%
\begin{table}[t]
    \centering
    \small
    \begin{tabular}{c|c|c}
    \textbf{Network Provision} & \textbf{Sharks} & \textbf{Dolphins}\\
    \hline\hline
    $5\mu$s, $40$Gbps & $35$\% & $20$\%\\\hline
    $3\mu$s, $100$Gbps & $30$\% & $15$\%\\\hline\hline
    \end{tabular}
    \caption{Sharks require slightly higher local memory than dolphins to be able to achieve an average performance penalty of $5\%$ for various latency-bandwidth configurations.}
    \label{tab:rmem}
\end{table}

%
\begin{table*}
  \centering
  \begin{tabular}{c|r|r|r|r|r|r}
		\multirow{2}{*}{\bf Component} & \multicolumn{2}{c|}{\bf Baseline} &  \multicolumn{2}{c|}{\bf With RDMA} &  \multicolumn{2}{c}{\bf With RDMA + NIC Integration}\\
				 &  {\bf (Across racks)} &  {\bf (Within rack)} & {\bf (Across racks)} &  {\bf (Within racks)} & {\bf (Across racks)} &  {\bf (Within racks)} \\\hline	\hline
    OS & $2 \times 0.95$ & $2 \times 0.95$ & $0$ & $0$ & $0$ & $0$\\\hline
    Data copy & $2 \times 2.00$ &  $2 \times 2.00$ & $2 \times 2.00$ & $2 \times 2.00$ & $2 \times 0.50$ & $2 \times 0.50$\\\hline
    Switching & $6 \times 0.24$ &  $2 \times 0.24$ & $6 \times 0.24$ & $2 \times 0.24$ & $6 \times 0.24$ & $2 \times 0.24$ \\\hline
    Propagation & $4 \times 0.20$ & $0$ & $4 \times 0.20$ & $0$ & $4 \times 0.20$ & $0$\\
    (Inter-rack) &  &   & & & &\\\hline
    Propagation & $4 \times 0.02$ & $4 \times 0.02$ & $4 \times 0.02$ & $4 \times 0.02$ & $4 \times 0.02$ & $4 \times 0.02$\\
    (Intra-rack) &  &   & & & \\\hline
    Transmission & $1 \times 0.82$ & $1 \times 0.82$ & $1 \times 0.82$ & $1 \times 0.82$ & $1 \times 0.82$ & $1 \times 0.82$\\\hline
    \hline
    {\bf Total} & {\bf $9.04\mu$s} & {$7.28\mu$s} & {$7.14\mu$s} & {$5.38\mu$s} & {\bf $4.14\mu$s} & {\bf $2.38\mu$s}\\\hline
	\hline
  \end{tabular}
  \vspace{0.1in}
  \caption{\small{Achievable round-trip latency (Total) and the breakdown into individual components that contribute to the round-trip latency (see discussion in \S\ref{ssec:rtt}) on a network with $40$Gbps access link bandwidth (one can further reduce the {\bf Total} by $0.5\mu$s using $100$Gbps access link bandwidth). The baseline denotes the latency achievable with existing network technology. The fractional part in each cell is the latency for one traversal of the corresponding component and the integral part is the number of traversal performed in one round-trip time (see discussion in \S\ref{ssec:rtt}).}}
  \label{tab:latency}
\end{table*}
%
\subsection{Implications and Feasibility}
\label{ssec:rtt}

We now examine the feasibility of meeting the requirements identified above. 

\paragraphb{Local memory} We start with the requirement of between $20-30$\% local memory. In our experiments, this corresponds to between $1.50-2.25$GB/core. We look to existing hardware prototypes for validation of this requirement. The Firebox prototype targets $128$GB of local memory shared by $100$ cores leading to $1.28$GB/core,\footnote{We thank Krste Asanovi{\'c} for clarification on the technical specifications of Firebox.} while the analysis in ~\cite{ddcHwDesign1} uses $1.5$GB/core. Thus we conclude that our requirement on local memory is compatible with demonstrated hardware prototypes.
Next, we examine the feasibility of meeting our targets for network bandwidth and latency.


\paragraphb{Network bandwidth} Our bandwidth requirements are easily met: $40$Gbps is available today in commodity datacenter switches \emph{and} server NICs~\cite{40gnic}; in fact, even $100$Gbps switches and NICs are available, though not as widely~\cite{100gnic}.
Thus, ignoring the potential effects of congestion (which we consider next in \S\ref{sec:existing}), providing the network bandwidth needed for disaggregation should pose no problem. Moreover, this should continue to be the case in the future because the trend in link bandwidths currently exceeds that in number of cores~\cite{hmc1, hmc2, hmc3}.


\paragraphb{Network latency} The picture is less clear with respect to latency. In what follows, we consider the various components of network latency and whether they can be accommodated in our target budget of 3$\mu$secs (for sharks) to 5$\mu$secs (for dolphins).

Table~\ref{tab:latency} lists the six components of the end-to-end latency incurred when fetching  a $4$KB page using $40$Gbps links, together with our estimates for each. Our estimates are based on the following common assumptions about existing datacenter networks: (1) the one-way path between servers in different racks crosses three switches (two ToR and one fabric switch) while that between servers in the same rack crosses a single ToR switch, (2) inter-rack distances of $40$m and intra-rack distances of $4$m with a propagation speed of $5$ns/m, (3) cut-through switches.\footnote{As before, we ignore the queueing delays that may result from congestion at switches -- we will account for this in \S\ref{sec:existing}.}  
With this, our round-trip latency includes the software overheads associated with moving the page to/from the NIC at both the sending and receiving endpoints (hence 2x the OS and data copy overheads), 6 switch traversals, 4 link traversals in each direction including two intra-rack and two cross-rack, and the transmission time for a 4KB page (we ignore transmission time for the page request), leading to the estimates in Table~\ref{tab:latency}. 

We start by observing that the network introduces three unavoidable latency overheads: (i) the data transmission time, (ii) the propagation delay; and (iii) the switching delay. 
Together, these components contribute to roughly $3.14\mu$s across racks and $1.38\mu$s within a rack.\footnote{Discussions with switch vendors revealed that they are approaching the fundamental limits in reducing switching delays (for electronic switches), hence we treat the switching delay as unavoidable.} 
%, assuming cut-through switching as is the norm in commodity datacenter switches~\cite{arista,broadcom}.

In contrast, the network software at the endpoints is a significant contributor to the end-to-end latency! Recent work report a round-trip kernel processing time of $950$ ns measured on a $2.93$GHz Intel CPU running FreeBSD (see ~\cite{luigi} for details), while ~\cite{ramcloud} report 
an overhead of around $2\mu$s to copy data between user space and the NIC.
With these estimates, the network software contributes roughly $5.9\mu$s latency --- this represents $65\%$ of the end-to-end latency in our baseline inter-rack scenario and $81\%$ in our baseline intra-rack scenario.

The end-to-end latencies we estimated in our baseline scenarios (whether inter- or intra-rack) 
fail to meet our target latencies for either dolphin or shark applications. Hence, we consider potential optimizations and technologies that can reduce these latencies. 
Two recent/emerging technologies show promise: RDMA and integrated NICs.

%\rc{Luigi}~\cite{luigi} evaluate this latency component for a server running FreeBSD HEAD %$64$bit OS on a i$7-870$ at $2.93$GHz + Turboboost CPU with Intel $10$Gbit NIC and ixgbe %driver. The evaluation shows that the {\em average} kernel packet processing time is roughly %$950$ns at each server, leading to $1.9\mu$s for a round trip time. 
%Once the packet is processed, existing commodity NICs require copying the data from the user space to the NIC buffer, which is estimated to be around $2\mu$s per server~\cite{ramcloud}. 



\paragraphb{\emph{Using RDMA}}
RDMA effectively bypasses the packet processing in the kernel, thus eliminating the OS overheads from Table~\ref{tab:latency}. Thus, using RDMA, we estimate a reduced end-to-end latency of $7.14\mu$s across racks (column\#4 in Table~\ref{tab:latency}) 
and $5.38\mu$s within a rack.

\paragraphb{\emph{Using NIC integration}}
Recent industry efforts pursue the integration of NIC functions closer to the CPU~\cite{cpu-nic} which would reduce the overheads associated with copying data to/from the NIC. Rosenblum {\it et al.}~\cite{lowlatency} estimate that such integration together with certain software  optimizations can reduce copy overheads to sub-microseconds, which we estimate at $0.5\mu$s (similar to~\cite{lowlatency}). 
%data copy from user space to NIC buffers can be reduced to sub-microseconds with the integration of NIC functions into the CPU and software optimizations~\cite{lowlatency}. 

\paragraphb{\emph{Using RDMA and NIC integration}}
As shown in column\#5 in Table~\ref{tab:latency}, the use of RDMA together with NIC integration reduces the end-to-end latency to $4.14\mu$s across racks; within a rack, this further reduces down to $2.38\mu$s (using the same differences as in column\#2 and column\#3).

\paragraphb{Takeaways} We highlight a few takeaways from our analysis:

\begin{itemize}[leftmargin=*]
\itemsep0em
	\item The overhead of network \emph{software} is the key barrier to realizing disaggregation with current networking technologies. Technologies such as RDMA and integrated NICs that eliminate some of these overheads offer promise: reducing end-to-end latencies to $4.14\mu$s between racks and $2.38\mu$s within a rack. However, demonstrating such latencies in a working prototype remains an important topic for future exploration.
	\item Even assuming RDMA and NIC integration, the end-to-end latency across racks ($4.14\mu$s) meets our target latency only for dolphin, but not shark, applications. Our target latency for sharks is only met by the end-to-end latency within a rack. Thus, shark jobs will have to be scheduled within a single rack (or nearby racks).
	That is, while dolphin jobs can be scheduled at blades distributed across the datacenter, shark jobs will need to be scheduled within a rack. The design and evaluation of such schedulers remains an open topic for future research.
	%Finally, even with kernel bypass, CPU-NIC integration and $100$Gbps link bandwidths, we will need intelligent schedulers for resource disaggregated datacenters. 
	\item While new network hardware such as high-bandwidth links (e.g., 100Gbps or even 1Tbps as in~\cite{vladimir, firebox}) and high-radix switches (\eg, $1000$ radix switch~\cite{firebox})  are certainly useful, they optimize a relatively small piece of the overall latency in our baseline scenario technologies. All-optical switches also fall into this category -- providing both potentially negligible switching delay and high bandwidth. That said, once we assume the benefits of RDMA and NIC integration, then the contribution of new links and switches could bring even the cross-rack latency to within our 3$\mu$sec target for shark applications, enabling true datatcenter-scale disaggregation; e.g., using $100$Gbps links reduces the end-to-end latency to $3.59\mu$s between racks, extremely close to our 3$\mu$secs. 
	%An immediate take-away is that optimizing the network software stack can lead to significantly more gains that optimizing the network hardware. In particular, using kernel bypass and CPU-NIC integration, the end-to-end latency can be reduced down to  Thus, while there is reason to believe that software overheads can be optimized to fit within our target latency budget, 
	%at the end-hosts leads to significant reduction in end-to-end latency compared to optimizations in the network fabric. Furthermore, the transmission time can be reduced to a minimal ($0.03\mu$s for $4$KB data) using $1$Tbps links. Overall, we estimate that the fundamental limits to end-to-end latency is $3.57\mu$s for inter-rack communication and $1.41\mu$s for intra-rack communication.
	\item Finally, we note that managing network congestion to achieve zero or close-to-zero queueing within the network will be essential; e.g., a packet that is delayed such that it is queued behind (say) 4 packets will accumulate an additional delay of $4\times0.82\mu$s! 
	Indeed, reducing such transmission delays may be the reason to adopt high-speed links. We evaluate the impact of network congestion in the following section.
	%SR: confused myself about this: hat necessitates One possibility is to consider adopting $100$Gbps technology \emph{not} because we need the higher capacity but instead to reduce transmission and queueing times. Using $100$Gbps links, the end-to-end latency could be reduced to $3.59\mu$s between racks and $1.83\mu$s within a rack, extremely close to our desired latency bounds.

% 	\item Somewhat surprisingly, the scope of disaggregation (i.e., rack-scale vs. datacenter-wide) is not an important factor in the feasibility of scaling disaggregation; instead, network bandwidth and an application's memory access pattern play a more important role. Our evaluation suggests that disaggregation at the scale of an entire datacenter is possible.
\end{itemize} 

\begin{figure}
  \centering
    \includegraphics[width=\columnwidth]{img/mem_bandwidth.pdf} 
    \caption{\small{\rc{memory bandwidth caption}}}
  \label{fig:memband}
\end{figure}

% An immediate takeaway is that managing network congestion to achieve zero or close-to-zero queueing delay will be essential. This need is exacerbated by the 
% non-trivial role of transmission delays (compared to classic wide-area contexts where propagation delay dominates): e.g., a packet that is delayed such that it incurs a store-and-forward delay at each switch along a 5-hop path will accumulate an additional $4\mu$s of delay just in transmission time! 
\cut{
We examine the feasibility of achieving zero-queueing in the following sections. 

One might consider two possibilities to optimizing for latency. First, adopting $100$Gbps technology \emph{not} because we need the higher capacity but instead to reduce transmission and queueing times. E.g., in the event of congestion as in our example above, the packet's transmission time overheads would be reduced from $4\mu$s to $1.6\mu$s. 
The other option is to consider limiting the scope of disaggregation, for example, to within a single rack. Somewhat surprisingly, the potential benefits of this appear modest: the primary impact (to latency) of limiting the scope of disaggregation is to reduce the propagation delay but this already only consumes $1\mu$s from our budget.
(Again, this ignores the potential benefits due to reduced congestion in a rack-scale architecture, which we consider in \S\ref{sec:existing}.)

Finally, a key latency component will arise from software overheads 
at the endpoints. Research prototypes in Xen have reported per-page overheads of $2-6\mu$s, depending on the page replacement algorithm~\cite{ddcHwDesign1} while recent work has argued that such overheads can be reduced to sub-microseconds with the integration of NIC functions into the CPU and software optimizations~\cite{lowlatency}. Thus, while there is reason to believe that software overheads can be optimized to fit within our target latency budget, demonstrating this remains an important topic for future exploration.

\paragraphb{Implication \#1:}  The bandwidth requirements for resource disaggregation are within the reach of existing technology. The key enabling factor is a network fabric with low ($\sim 5\mu$s) end-to-end latency.

\vspace{0.1in}
\noindent
%We now discuss the feasibility of a network fabric that provides the above end-to-end latency. Indeed, 
There are two unavoidable contributors to end-to-end network latency --- transmission time for packets in the flow, and propagation delay. \sr{offer evidence??} Given that propagation delays are extremely small in existing datacenters, achieving low end-to-end network latency boils down to minimizing the transmission time for packets in the flow. This has several implications for design of \dis:

\paragraphb{Implication \#2:} Applications running atop \dis must observe little or no queueing delays (that is, no network congestion). 

\vspace{0.1in}
\noindent
The queueing delays observed by flow packets depend heavily on both the network traffic as well as network protocols. We return to evaluating the feasibility of achieving little or no queueing delays in \S\ref{sec:existing}. {motivate via Figure~\ref{fig:impb} and Figure~\ref{fig:impl} $\to$} Note that $100$Gbps links are already available at both switch ports and as server NICs, but are not commonly deployed. Transmission delays dominating the end-to-end network latency have the following implication in the context of \dis:
}

\cut{
\paragraphb{Implication \#3:} While capacity constraints do not impose strict bandwidth requirements, one good reason to consider $100$Gbps links is reducing end-to-end network latency via reduction in transmission time. 

\vspace{0.1in}
\noindent
 Finally, propagation delays having little impact on end-to-end network latency has an interesting implication in \dis design:

\paragraphb{Implication \#4:} Ignoring congestion, the scale of disaggregation (rack-, pod- or datacenter-scale) may have little impact on application-level performance, and disaggregation at the extreme of datacenter-scale may indeed be feasible.
}

\cut{
\vspace{0.1in}
\noindent
We evaluate the application-level performance for rack- and datacenter-scale resource disaggregation in \S\ref{sec:existing}. 
}

% While $100$Gbps network bandwidth does not provide significant benefits over the $40$Gbps case, reducing the latency down to $1\mu$s can lead to applications observing essentially no performance degradation. In the hindsight, this is not surprising given our results from \S\ref{sec:workloads}, where we established that the network traffic volume does not increase significantly in \dis compared to \pdis, and that the network flows are dominated by short (latency-sensitive) memory access flows. \rc{$\gets$ needs more concrete intuition; remote memory faster than local disk; applications heavily pipelined; CPU bottleneck?}

%\paragraphb{Benefits of remote memory}
%First, given sufficient network bandwidth and small network latencies, use of remote memory can drastically improve application performance compared to traditional disk-based swap. Since the working set size is hard to predict in advance, memory tends to be highly over-provisioned in datacenter servers to prevent thrashing. Disaggregated remote memory can reduce this waste by providing an elastic memory capacity pooled at the datacenter scale. 

% \paragraphb{Reducing latency more important than increasing bandwidth}
% Second, low latency is more important than high bandwidth. The $100$Gbps bandwidth did not provide any significant improvement over the $40$Gbps link. In contrast, $10\mu$s round-trip latency causes noticeable performance degradation, as compared to the $1\mu$s case.


%
% \begin{figure}
%   \centering
%     \includegraphics[width = 3.5in]{img/vary_remote_mem.eps} 
%   \caption{\small{\rqc{Fix latency at 5us, bandwidth at 40Gbps, Vary local memory from 0\% to 90\% of the total memory of the EC2 instance}}}
%   \label{fig:impb}
% \end{figure}
%

% graveyard of text scraps.
\cut{
from the network such that the application-level performance in \dis is comparable to that in server-centric architectures.
Overall, our results show that:

\begin{itemize}[leftmargin=*]
	\itemsep0em
	\item CPU blades having local cache is necessary to ensure reasonable performance, given the increased latency to access remote memory. However, $25\%$ of main memory serving as local cache suffices.
	\item Applications require non-trivial latency performance ($5$--$10\mu$s) from the network. In contrast, $40$Gbps access link capacity suffices. Moreover, a good reason to consider higher link capacity is not to handle higher traffic volume, but to reduce the transmission time at the end-hosts.  
    \item Ignoring congestion, the scale of disaggregation (rack versus datacenter scale) has little impact on application-level performance. \rc{$\gets$ right now, we have this results in S5 using simulations for rack and datacenter scale disagg; how should we inject latencies to make this point?}
\end{itemize}

\noindent
}

%Depending on the page eviction algorithm used, 
 %existing research prototypes report this per-page overhead to be roughly in the range of $2$--$6\mu$s~\cite{x1, x2, x3}. However, this overhead can be reduced to sub-microseconds with faster CPUs and software optimization~\cite{y1, y2, y3}, making this overhead insignificant. 
