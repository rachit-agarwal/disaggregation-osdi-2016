\vspace{-0.1in}
\section{Future Directions}
\label{sec:future}
\vspace{-0.05in}

So far, we used emulation and simulation to evaluate the minimum network requirements for \dis disaggregation, with a focus on minimizing degradation in application performance. 
This opens two directions for future work that we briefly investigate here: (1) demonstrating an end-to-end system implementation of remote memory access that meets our latency targets, and (2) investigating programming models that actively exploit disaggregation to \emph{improve} performance.
Our results in this section are merely intended to demonstrate the potential for realizing positive results to the above questions: each topic merits an in-depth exploration that is out of scope for this paper.


\vspace{-0.1in}
\subsection{Implementing remote memory access}
\vspace{-0.05in}
We previous identified an end-to-end latency target of 3-5us for \dis and argued for the use of  RDMA to meet this target. The (promising) RDMA latencies we assumed in \S\ref{sec:existing} were based on measurements reported by native RDMA-based applications. 
We were curious about the feasibility of realizing these latencies if we were to retain our 
architecture from the previous section in which remote memory is accessed as a special swap device as this would provide a simple and transparent approach to utilizing remote memory. 

We thus built a kernel space RDMA block device driver which serves as a swap device; i.e., the local CPU can now swap to remote memory instead of disk.
We implemented the block device driver on a machine with Mellanox  4xFDR Infiniband card.
We test the block device throughput using \textit{dd} with direct IO, and measure the request latency by instrumenting the driver code. 
The end-to-end latency of our approach includes the RDMA request latency and the latency introduced by the kernel swap itself. We focus on each in turn. 

\paragraph{RDMA request latency.} A few optimizations were necessary to improve RDMA performance in our context. First, we batch block requests to be sent to the RDMA NIC and the driver waits for all the requests to return before notifying the upper layer: this gave a block device throughput of only 0.8GB/s and latency around 4-16us.
Next, we observed that many block requests have contiguous addresses and hence merge these requests into a single large request: this improved throughput to 2.6GB/s (a ~3x improvement).
Finally, we improved performance by issuing RDMA requests asynchronously. We created a data structure to keep track of all outgoing requests and notify the upper layer immediately for each completed request: this improves the throughput to 3.3GB/s which is as high as a local RamFS, and reduces the request latency to 3-4us (Table \ref{tab:rdma_latency}). 
This latency is within 2x of request latencies reported by native RDMA applications which we view as encouraging given the simplicity of the design and that additional optimizations are likely possible.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{ccccc}
    \textbf{Min}	& \textbf{Avg}	& \textbf{Median} & \textbf{99.5 Pcntl}	& \textbf{Max}\\
    \hline
    3394 &	3492&	3438&	4549&	12254\\\hline
    \end{tabular}
    \caption{RDMA block device request latency(ns)}
    \label{tab:rdma_latency}
\end{table}

In current non-NUMA architecture, Linux Kernel only launches one \textit{kswapd} to swap out least recently used pages. 
This could be a bottleneck for apps that heavily use remote memory.
We enable the NUMA\_EMU feature in Linux kernel, which partitions the memory into several regions, and each memory region gets its own \textit{kswapd}. 
This improves the throughput of swapping out memory pages.


\eat{
The RDMA block device driver provides us a simple and efficient way utilize remote memory. 
We setup the block device as swap space and run the \rc{5} applications evaluated in \S \ref{sec:requirements} with lower latency requirements.
\rc{waiting for joao's results}

}


\paragraphb{Swap latency}
We measured the software overhead of swapping on a commodity desktop running
Linux 3.13.0. For this, we must collect (and separate out) two data points with
minimal modifications to the codebase --- the overall time spent in the page
fault handler on a swap and the time spent actually accessing disk. We found that 
convenient measurement tools such as \texttt{ftrace} and \texttt{printk} 
introducedunacceptable overhead for our purposes. 
%We also could not arbitrarily introduce
%\texttt{printk} calls within the areas we were measuring, since \texttt{printk}
%introduces a noticable overhead when measuring on the order of microseconds.
Instead, we took advantage of the fact that the
\texttt{handle\_mm\_fault} function called by the page fault handler (and which
eventually handles swaps) never utilizes the upper 16 bits of its 32 bit return
value (as evidenced by the \texttt{VM\_FAULT\_*} macros in the \texttt{mm.h}
header). Thus, we wrap both the body of the \texttt{\_\_do\_page\_fault} function and the
call to the \texttt{swapin\_readahead} function (which performs a swap from
disk) in ktime\_get calls.  We then pack the result of the measurement for the
swapin\_readahead function into the upper 16-bits of its caller,
\texttt{do\_swap\_page}, which propagates the value up to
\texttt{\_\_do\_page\_fault}. 
Once we have measured the body of
\texttt{\_\_do\_page\_fault}, we record both the latency of the whole
\texttt{\_\_do\_page\_fault} routine, as well as the time spent in
\texttt{swapin\_readahead}. We subtract these and average to find that the
software overhead of page fault handler is 2.47 $\mu$s. This
number is a lower-bound on the software overhead of the handler, because we
assume that all of \texttt{swapin\_readahead} is a ``disk access'' (we cannot neatly instrument inside \texttt{swapin\_readahead} in the same manner since it returns a pointer).

In combination with the RDMA request latencies, we find that a system implementation  
of 

%Overall, our measurement code introduces a very small number of bitwise
%operations (ANDs, ORs, shifts), one integer subtraction, and four calls to
%\texttt{ktime\_get} into the handler, which together have a negligible impact on
%measurements.

