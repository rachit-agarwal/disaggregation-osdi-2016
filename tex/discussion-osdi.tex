\vspace{-0.1in}
\section{Future Directions}
\label{sec:future}
\vspace{-0.05in}

So far, we used emulation and simulation to evaluate the minimum network requirements for disaggregation, with a focus on minimizing degradation in application performance. 
This opens two directions for future work that we briefly investigate here: (1) demonstrating an end-to-end system implementation of remote memory access that meets our latency targets, and (2) investigating programming models that actively exploit disaggregation to \emph{improve} performance.
Our results in this section are merely intended to demonstrate the potential for realizing positive results to the above questions: each topic merits an in-depth exploration that is out of scope for this paper.


\vspace{-0.1in}
\subsection{Implementing low-latency remote memory access}
\vspace{-0.05in}
We previous identified an end-to-end latency target of 3-5us for \dis and argued for the use of  RDMA to meet this target. The (promising) RDMA latencies we assumed in \S\ref{sec:existing} were based on measurements reported by native RDMA-based applications. 
We were curious about the feasibility of realizing these latencies if we were to retain our 
architecture from the previous section in which remote memory is accessed as a special swap device as this would provide a simple and transparent approach to utilizing remote memory. 

We thus built a kernel space RDMA block device driver which serves as a swap device; i.e., the local CPU can now swap to remote memory instead of disk.
We implemented the block device driver on a machine with Mellanox  4xFDR Infiniband card.
We test the block device throughput using \textit{dd} with direct IO, and measure the request latency by instrumenting the driver code. 
The end-to-end latency of our approach includes the RDMA request latency and the latency introduced by the kernel swap itself. We focus on each in turn. 

\paragraph{RDMA request latency.} A few optimizations were necessary to improve RDMA performance in our context. First, we batch block requests to be sent to the RDMA NIC and the driver waits for all the requests to return before notifying the upper layer: this gave a block device throughput of only 0.8GB/s and latency around 4-16us.
Next, we observed that many block requests have contiguous addresses and hence merge these requests into a single large request: this improved throughput to 2.6GB/s (a ~3x improvement).
Finally, we improved performance by issuing RDMA requests asynchronously. We created a data structure to keep track of all outgoing requests and notify the upper layer immediately for each completed request: this improves the throughput to 3.3GB/s which is as high as a local RamFS, and reduces the request latency to 3-4us (Table \ref{tab:rdma_latency}). 
This latency is within 2x of request latencies reported by native RDMA applications which we view as encouraging given the simplicity of the design and that additional optimizations are likely possible.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{ccccc}
    \textbf{Min}	& \textbf{Avg}	& \textbf{Median} & \textbf{99.5 Pcntl}	& \textbf{Max}\\
    \hline
    3394 &	3492&	3438&	4549&	12254\\\hline
    \end{tabular}
    \caption{RDMA block device request latency(ns)}
    \label{tab:rdma_latency}
\end{table}

In current non-NUMA architecture, Linux Kernel only launches one \textit{kswapd} to swap out least recently used pages. 
This could be a bottleneck for apps that heavily use remote memory.
We enable the NUMA\_EMU feature in Linux kernel, which partitions the memory into several regions, and each memory region gets its own \textit{kswapd}. 
This improves the throughput of swapping out memory pages.


\eat{
The RDMA block device driver provides us a simple and efficient way utilize remote memory. 
We setup the block device as swap space and run the \rc{5} applications evaluated in \S \ref{sec:requirements} with lower latency requirements.
\rc{waiting for joao's results}

}