\vspace{-0.1in}
\section{Disaggregated Datacenters}
\label{sec:summary}
\vspace{-0.05in}

\rc{cut: background subsection, folded into 2.1 and section intro}

\begin{table}
  \centering
  \small
  \begin{tabular}{l|c|c}
		\textbf{Communication} & \textbf{Latency (ns)} & \textbf{Bandwidth (Gbps)}\\\hline
	\hline
    CPU -- CPU & $10$ & $500$\\\hline
    CPU -- Memory & $20$ & $500$\\\hline
    CPU -- Disk (SSD) & $10^4$ & $5$\\\hline
    CPU -- Disk (HDD) & $10^6$ & $1$\\\hline
    \hline
  \end{tabular}
  \vspace{0.1in}
  \caption{\small{Typical latency and peak bandwidth requirements within a traditional server. Numbers vary between hardware.}}
  \label{tab:tech}
\end{table}

The high-level idea behind a disaggregated datacenter is illustrated in Figure~\ref{fig:dc}.
A \dis comprises standalone hardware ``blades'' for each resource type,  interconnected by a network fabric.
Multiple prototypes of disaggregated hardware already exist --- Intel RSA~\cite{rsa}, HP ``the machine''~\cite{hptm}, Facebook's disaggregated rack~\cite{fdr}, Huawei's DC3.0~\cite{huawei}, and SeaMicro~\cite{seamicro}, as well as research prototypes like Firebox~\cite{firebox}, soNUMA~\cite{sonuma}, and memory blades~\cite{ddcHwDesign1}. 
Many of these systems are proprietary and/or in the early stages of development; nonetheless, in our study we draw from what information is publicly available to both borrow from and critically explore the design choices made by existing hardware prototypes.

In this section, we present our assumptions regarding the hardware (\S\ref{ssec:hardware}) and system (\S\ref{ssec:system}) architecture in a disaggregated datacenter.
We close the section by summarizing the key open design choices that remain after our assumptions (\S\ref{ssec:knobs}); we treat these as design `knobs' in our evaluation.

\vspace{-0.1in}
\subsection{Assumptions: Hardware Architecture}
\vspace{-0.05in}
\label{ssec:hardware}

\paragraphb{(1) Partial CPU-memory disaggregation} 
In general, disaggregation suggests that each blade is comprised of a particular resource with a direct interface to the network fabric (Fig.~\ref{fig:dc}). 
One exception to this strict decoupling are CPU blades: each CPU blade retains some amount of \emph{local} memory that acts as a cache for remote memory dedicated for cores on that blade\footnote{We use ``remote memory'' to refer to the memory located on a standalone memory blade.}.
Thus, CPU-memory disaggregation can be viewed as expanding the memory hierarchy to include a remote level, which is shared by all CPU blades. 

This architectural choice is reported in prior work~\cite{firebox, ddcHwDesign1, ddcHwDesign2, firebox, huawei}. 
While we assume that partial CPU-memory disaggregation will be the norm, we go a step further and evaluate how the amount of local memory impacts \emph{network} requirements in terms of network bandwidth and latency, and transport-layer flow completion times.
	
\paragraphb{(2) Cache coherence domain is limited to a single compute blade} As articulated by others~\cite{firebox,hptm,huawei}, this has the important implication that CPU-to-CPU cache coherence traffic does not hit the network fabric.
This assumption is necessary because an external network fabric is unlikely to support the latency and bandwidth requirements for inter-CPU cache coherence (Table~\ref{tab:tech}).

\paragraphb{(3) Resource Virtualization}
Each resource blade must support virtualization of its resources; this is necessary for resources to be logically aggregated into higher-level abstractions such as VMs or containers.
Virtualization of IO resources is widely available even today: many IO device controllers now support virtualization via PCIe, SR-IOV, or MR-IOV features~\cite{sriov} and the same can be leveraged to virtualize IO resources in \dis.
The disaggregated memory blade prototyped by Lim \etal~\cite{ddcHwDesign1} includes a controller ASIC on each blade that implements address translation between a remote CPU's view of its address space and the addressing used internally within the blade. Similar designs are assumed by other research efforts. We note that while the implementation of such blades may require some additional new hardware, it requires no change to existing components such as CPUs, memory modules, or storage devices themselves.	

\paragraphb{(4) Rack-scale disaggregation} Existing prototypes limit the scope of disaggregation to a very small number of racks. For example, Firebox~\cite{firebox} envisions a single system as spanning approximately three racks and assumes that the \emph{logical} aggregation and allocation of resources is similarly scoped; i.e., the resources allocated to a higher-level abstraction such as a VM or a container are selected from a single firebox. Similarly, the scope of disaggregation in Intel's RSA is a single rack~\cite{rsa}. In contrast, in a hypothetical datacenter-scale disaggregated system, resources assigned to (for example) a single VM could be selected from anywhere in the datacenter.  
%
\begin{table*}[t]
  \centering
  \small
  \begin{tabular}{c|c|c|c}
		\textbf{Application Domain} & \textbf{Application} & \textbf{System} & \textbf{Dataset} \\\hline \hline
		
    Off-disk Batch & WordCount & Hadoop & Wikipedia edit history~\cite{wikipedia}\\
     Off-disk Batch & Sort & Hadoop & Sort benchmark generator\\
     Graph Processing & Collaborative Filtering & GraphLab & Netflix movie rating data~\cite{netflix}\\
    Point Queries & Key-value store & Memcached & YCSB\\
    Streaming Queries & Stream WordCount & Spark Streaming & Wikipedia edit history~\cite{wikipedia}\\
    \hline
     \hline
     
    In-memory Batch & WordCount & Spark & Wikipedia edit history~\cite{wikipedia}\\
     In-memory Batch & Sort & Spark & Sort benchmark generator\\
     Parallel Dataflow & Pagerank & Timely Dataflow & Friendster Social Network~\cite{friendster}\\
     In-memory Batch & SQL & Spark SQL & Big Data Benchmark~\cite{bdb}\\
     Point Queries & Key-value store & HERD & YCSB\\\hline

    \hline
  \end{tabular}
  \vspace{0.1in}
  \caption{\small{Applications, workloads, systems and datasets used in our study.}}
  \label{tab:workloads}
\end{table*}
%

\paragraphb{(5) Specialized network designs} 
Corresponding to their assumed scope of disaggregation, existing prototypes assume a different network architecture for within the rack(s) that form a unit of disaggregation vs. between such racks. To our knowledge, all existing \dis prototypes use specialized -- even proprietary~\cite{seamicro,huawei,rsa} -- network technologies and protocols within a disaggregated rack(s). For example, SeaMicro uses a proprietary Torus-based topology and routing protocol within its disaggregated system; Huawei propose a PCIe-based fabric~\cite{huawei1}; Firebox assumes an intra-Firebox network of 1Tbps Silicon photonics links interconnected by high-radix switches~\cite{firebox,vladimir}; and Intel's RSA likewise explores the use of Silicon photonics links and switches. 

Rather than simply accepting the last two design choices (rack-scale disaggregation and specialized network designs), we critically explore when and why these choices are necessary. Our rationale in this is twofold. 
First, these are both choices that appear to be motivated not by fundamental constraints around disaggregating memory or CPU at the hardware level, but rather by the assumption that existing networking solutions cannot meet the (bandwidth/latency) requirements that disaggregation imposes on the network. However, to our knowledge, there has been no systematic evaluation showing this to be the case; hence, we seek to develop quantifiable arguments that either confirm or refute the need for these choices. 

Second, these choices are likely to complicate or delay the deployment of \dis. The use of a different network architecture within vs. between disaggregated islands leads to the complexity of a two-tier heterogeneous network architecture with different protocols, configuration APIs, etc., for each; e.g., in the context of their Firebox system, the authors envisage the use of special gateway devices that translate between their custom intra-Firebox protocols and TCP/IP that is used between Firebox systems; Huawei's DC3.0 makes similar assumptions. Likewise, many of the specialized technologies these systems use (e.g., Si-photonics~\cite{vladimir-nature}) are still far from mainstream. 
Hence, once again, rather than assume change is necessary, we evaluate the possibility of maintaining a uniform ``flat'' network architecture based on existing commodity components as advocated in prior work~\cite{el-fares,vl2,greenberg-costs}. 
%


\vspace{-0.1in}
\subsection{Assumptions: System Architecture}
\vspace{-0.05in}
\label{ssec:system}
In contrast to our assumptions regarding hardware which we based on existing  prototypes, we have less to guide us on the systems front. We thus make the following assumptions, which we believe are reasonable: 

\paragraphb{System abstractions for \emph{logical} resource aggregations} 
In a \dis, we will need system abstractions that represent a logical aggregation of resources, in terms of which we implement resource allocation and scheduling. 
One such abstraction in existing datacenters is a VM: 
operators provision VMs to aggregate slices of hardware resources within a server, and schedulers place jobs across VMs. 
While not strictly necessary, we note that the VM model can still be useful in \dis.\footnote{In particular, continuing with the abstraction of a VM would allow existing software infrastructure --- i.e., hypervisors, operating systems, datacenter middleware, and applications --- to be reused with little or no modification.} 
For convenience, in this paper we assume that computational resources are still aggregated to form VMs (or VM-like constructs), although now the resources assigned to a VM come from distributed hardware blades. Given a VM (or VM-like) abstraction, we assign resources to VMs differently based on the \emph{scope} of disaggregation that we assume: for rack-scale disaggregation, a VM is assigned resources from within a single rack while, for datacenter-scale disaggregation, a VM is assigned resources from anywhere in the datacenter.

\paragraphb{Hardware organization} 
We assume that resources are organized in racks as in today's datacenters. 
We assume a `mixed' organization in which each rack hosts a mix of different types of resource blades, as opposed to a `segregated' organization in which a rack is populated with a single type of resource (e.g., all memory blades). 
This leads to a more uniform communication pattern which should simplify network design and also permits optimizations that aim to localize communication; e.g., co-locating a VM within a rack, which would not be possible with a segregated organization. 

\paragraphb{Page-level remote memory access} \rc{reviewer: why not cache-line access}
In traditional servers, the typical memory access between CPU and DRAM occurs in the unit of a cache-line size (64B in x86). However, we assume that CPU blades access remote memory at the granularity of a page (4KB in x86), since this requires little or no modification to the virtual memory subsystems of hypervisors or operating systems, and is completely transparent to user-level applications. Note that this assumption constitutes a bad-case scenario for our study since it significantly increases the amount of data read per memory access (and hence, latency for transferring this larger amount of data across the network) as well as the total traffic volume.

%. ; however, page-level access has been shown to better exploit spatial locality in common memory access patterns and hence amortizes the round-trip latency more effectively in the context of \dis ~\cite{ddcHwDesign1}. In addition, page-level access requires little or no modification to the virtual memory subsystems of hypervisors or operating systems, and is completely transparent to user-level applications. We further assume that those remotely accessed pages are not shared by multiple VMs at a given time, in order to not introduce cache coherence traffic across the network.

\paragraphb{Block-level distributed data placement}
We assume that applications in \dis read and write large files at the granularity of ``sectors'' ($512$B in x86). Furthermore, the disk block address space is range partitioned into ``blocks'', that are uniformly distributed across the disk blades. The latter is partially motivated by existing distributed file systems (\eg, HDFS) and also enables better load balancing. % because this spreads load. 

\vspace{-0.1in}
\subsection{Design knobs}
\label{ssec:knobs}
\vspace{-0.05in}
Given the above assumptions, we are left with two key system design choices that we treat as ``knobs'' in our study: {\em the amount of local memory on compute blades} and  {\em the scope of disaggregation} (e.g., rack- or datacenter-scale). We will explore how varying these knobs impacts the network requirements and traffic characteristics in \dis. 
The remainder of this paper is organized as follows. We first analyze network-layer bandwidth and latency requirements in \dis(\S\ref{sec:requirements}) \emph{without} considering contention between network flows, then in \S\ref{sec:existing} relax this constraint. We end with a brief discussion of the potential application performance benefits of disaggregation and related work in \S\ref{sec:discussion}.